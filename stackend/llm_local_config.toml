# Local LLM models configuration
#
# This file contains the configuration for the local LLM models that will be available when using StackAI.
#
# How to use this file:
# The set up process has two steps:
# 1. Add as many local LLM models as you intend to use (see commented example below)
# 2. Set up the default model in the [llms.providers.Local.default] section.
#    IMPORTANT: The model_name variable should be the name of the model in the api. For instance, if you were
#    to set up the sample llm below as your default, you should set `model_name` to "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
#    rather than "Meta Llama 3.1 Turbo"
#
# Common issues solved:
# - The provider name for the local llms is "Local", not "local"


[llms.providers.Local]
plan_required = "enterprise"
hosted_by = "Stack AI"
display_order = 13


# REFERENCE LOCAL MODEL CONFIGURATION:
#
# You may add as many local LLM models as you need.
# To do so, please use the configuration below as a reference and adjust it to your needs
# The most important values to set up are:
#
# - model name in the api: This needs to be put in the toml header [llms.providers.Local."YOUR MODEL NAME IN YOUR API"]
# - endpoint_base_url: The base URL of the OpenAI compatible API. This has to be set up in the toml body.
# - api_key: The API key for the OpenAI compatible API. This has to be set up in the toml body.
#
#
# [llms.providers.Local."meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"]
# endpoint_base_url = "https://api.together.xyz/v1"
# api_key = "example api key"
# name = "Meta Llama 3.1 Turbo"
# description = "This is a dummy configuration model that you can use as reference to create your own configuration."
# context_window = 1024
# rating_reasoning = 1
# rating_speed = 1
# rating_context = 1
# date = ""
# has_json_format = false
# has_json_schema = false
# has_vision = false
# supported_media_types = []\



[llms.providers.Local.generic_local]
name = "OpenAI API Compatible Model"
description = "A local model for testing and development. Go into the node settings and set up the endpoint base url and api key of the node for it to work."
context_window = 128000
rating_reasoning = 1
rating_speed = 1
rating_context = 1
date = ""
has_json_format = false
has_json_schema = false
has_vision = false
supported_media_types = []


# Remember to adjust this as well!
[llms.providers.Local.default]
model_id = "local"